{% extends "page.twig" %}

{% block content %}

    <p>To express textual variance, CollateX uses a graph-based data model
        (<a href="http://dx.doi.org/10.1016/j.ijhcs.2009.02.001" title="Schmidt, D. and Colomb, R, 2009. A data structure for representing multi-version texts online, International Journal of Human-Computer Studies, 67.6, 497-514.">Schmidt 2009</a>).
        On top of this model it supports several algorithms to progressively align multiple text versions.</p>


    <h2 id="gothenburg-model">Stages of the Collation Process: The Gothenburg Model</h2>

    <p>Developers of CollateX and
        <a href="http://www.juxtasoftware.org/">Juxta</a> met for the first time in&nbsp;2009 at a joint workshop of
        <a href="http://www.cost-a32.eu/">COST Action&nbsp;32</a> and
        <a href="http://www.interedition.eu/">Interedition</a> in Gothenburg. They started discussing, how the different concerns
        of computer-supported collation of texts could be separated such that these two as well as similar projects would have a common
        understanding of its process and could thus collaborate more efficiently on the development of collation tools
        as well as their components. As a first result of this ongoing discussion, the participants identified five distinct tasks
        present in any computer-supported collation workflow.</p>

    <p>CollateX is designed around this separation of concerns.</p>

    <h3 id="tokenization">Tokenization</h3>

    <div class="figure float-right">
        <img src="/images/tokenizer.png" alt="Tokenizer">
        <p class="caption">A tokenized text</p>
    </div>

    <p>While computers can compare a text's versions on a character-by-character basis, in the more common use case each
        version is first split up into parts &ndash; henceforth called <em>tokens</em> &ndash; so the comparison can be conducted
        on a more coarse-grained level where the tokens to be compared ideally correspond to the text's units which carry meaning.
        This pre-processing step is called <strong>tokenization</strong> and performed by a <em>tokenizer</em>; it can happen on any
        level of granularity, i.e. on the level of syllables, words, lines, phrases, verses, paragraphs or text nodes in a
        <a href="http://en.wikipedia.org/wiki/Document_Object_Model" title="Wikipedia Article">DOM</a>.</p>

    <p>Another service provided by tokenizers and of special value to the comparison of natural language texts relates to marked-up
        text versions: As most collation software primarily compares text versions based on their textual content, embedded markup
        would usually get in the way of this process and therefore needs to be discarded or “pushed in the background”, so the
        collation tool does not have to be concerned about the specifics of a text's encoding. At the same time it might be valuable
        to keep the markup context of every token for reference, for instance if one wanted to make use of it when comparing tokens.</p>

    <p>The figure to the right depicts this process: The line on top shows a marked-up text, its content as the
        characters "a", "b", "c" and "d" &ndash; each representing a token &ndash; and "e1", "e2" as examples of embedded markup elements.
        A markup-aware tokenizer would not only split this version into 4 distinct tokens but transform it into a sequence of such tokens,
        with each token referring to its markup context.</p>

    <p>For now CollateX offers a simple tokenizer, mainly serving prototyping purposes by either</p>

    <ul>
    <li>splitting plain text without any embedded markup on boundaries determined by
        <a href="http://en.wikipedia.org/wiki/Whitespace_character" title="Wikipedia Article">whitespace</a>, or</li>
    <li>evaluating a configurable <a href="http://www.w3.org/TR/xpath/" title="XPath 1.0 W3C Spec">XPath 1.0 expression</a> on an XML-encoded
        text version which yields a list of node values as textual tokens.</li>
    </ul>

    <p>While not offering a comprehensive tokenizer itself, CollateX can be combined with any such tool that suits your specific requirements.
        CollateX only expects you then to provide text versions in pre-tokenized form and define a <strong>token comparator function</strong> which &ndash; when
        called with any two tokens &ndash; evaluates to a <em>match</em> in case those two tokens shall  be treated as equal, or a <em>mismatch</em> in case
        this should not be assumed. Formally speaking, a token comparator function defines an
        <a href="http://en.wikipedia.org/wiki/Equivalence_relation" title="Wikipedia Article">equivalence relation</a> over all tokens for a
        collation. In processing tokens on the level of their equivalence defined by such a relation, CollateX is agnostic with regard to what constitutes
        a token in your specific use case, whether it is plain text, text with a markup context or not textual at all.</p>

    <p>Detailed information about when and how to define your own notion of a token and its corresponding equivalence relation will be given in
        the following sections on CollateX' usage. Its built-in tokenizer will provide for an easy start. Later on you can opt for a more versatile tokenizer
        and/or token comparator function in order to enhance the accuracy of collation results.</p>

    <h3 id="normalization">Normalization/Regularization</h3>

    <p>With a configurable equivalence relation between tokens (defined via the aforementioned comparator function), CollateX can compare
        text versions which are comprised of arbitrary tokens sequences. For a larger number of use cases though, this flexibility of defining a
        fully customized comparator function is not really needed. It might suffice to normalize the tokens' textual content
        such that an exact matching of the normalized content yields the desired equivalence relation. For instance, in many cases all tokens
        of the text versions are normalized to their lower-case equivalent before being compared, thereby making their comparison case insensitive. Other
        examples would be the removal of punctuation, the rule-based normalization of orthographic differences or the
        <a href="http://en.wikipedia.org/wiki/Stemming" title="Wikipedia Article">stemming of words</a>.</p>

    <p>Just as with the tokenizer included in CollateX, its normalization options are rather simple. Beyond the mentioned case normalization
        and the removal of punctuation and/or whitespace characters, CollateX does not include any sophisticated normalization routines. Instead its
        API and supported input formats provide the user with options to plug in their own components when needed.</p>

    <h3 id="alignment">Alignment</h3>

    <div class="figure float-left">
        <img src="/images/aligner.png" alt="Alignment">
        <p class="caption">An alignment of 3 versions/witnesses</p>
    </div>

    <p>After each version has been tokenized and optionally normalized, a collation tool will align the token sequences. Simply put, aligning the
        witnesses means in this case: Find matching tokens and insert empty tokens (<i>gap tokens</i>) such that the
        token sequences of all witness line up properly. Interestingly this problem is computationally similar to the
        problem of <a href="http://en.wikipedia.org/wiki/Sequence_alignment" class="external text" rel="nofollow">sequence
            alignment</a> encountered in bioinformatics.
    </p>

    <p>Looking at an example, assume that we have three witnesses: the first is comprised of the token sequence (a, b,
        c, d), the second reads (a, c, d, b) and the third (b, c, d). A collator might align these three witnesses as
        depicted in a tabular fashion on the right. Each witness occupies a column, matching tokens are aligned in a
        row, necessary gap tokens as inserted during the alignment process are denoted via a hyphen. Depending from
        which perspective one interprets this <i>alignment table</i>, one can say for example that the (b) in the second
        row was ommitted in the second witness or it has been added in the first and the third. A similar statement can
        be made about (b) in the last row by just inverting the relationship of being added/ommitted.
    </p>

    <p>Alignment tables like the one shown can be encoded losslessly with an existing apparatus encoding scheme, in
        parallel segmentation mode, as long as only the textual content of token needs to be represented. Each row is
        represented by a segment with empty readings for gap tokens. Optionally consecutive rows with identical readings
        for each witness can be compressed into a single segment, e.g.
    </p>

    <pre class="prettyprint clear">&lt;app&gt;
  &lt;rdg wit="#w1 #w2"&gt;a&lt;/rdg&gt;
  &lt;rdg wit="#w3" /&gt;
&lt;/app&gt;
&lt;app&gt;
  &lt;rdg wit="#w1 #w3"&gt;b&lt;/rdg&gt;
  &lt;rdg wit="w2" /&gt;
&lt;/app&gt;
&lt;app&gt;
  &lt;rdg wit="#w1 #w2 #w3"&gt;cd&lt;/rdg&gt;
&lt;/app&gt;
&lt;app&gt;
  &lt;rdg wit="#w2"&gt;b&lt;/rdg&gt;
  &lt;rdg wit="#w1 #w3" /&gt;
&lt;/app&gt;</pre>

    <h3 id="alignment-analysis">Analysis</h3>

    <div class="figure float-right">
        <img src="/images/analyzer.png" alt="Alignment Analysis">
        <p class="caption">Analyzing an alignment</p>
    </div>

    <p>On top of the results delivered by the alignment process, a further analysis can yield additional findings.
        Echoing the example from the above section, one might want to think of the token (b) in row 2 and 5 as being
        transposed instead of as being added/omitted separately. Some collators try to detect transpositions as part of
        the alignment process, some do it as a post-processing step and others do not handle transpositions at all
        and/or leave it to the user to declare those beforehand. Part of the reason for algorithmic differences in
        transpostion handling is the fact, that the question which tokens are actually transposed is much more a matter
        of interpretation than the question of matching and aligning them. While alignment results can still be judged
        in terms of their quality to some extent, transposition detection can only be done heuristically as one can
        easily think of cases, where it is impossible for a computer “to get it right”.
    </p>

    <p>Apart from the specific problem of transpositions, it seems generally necessary to incorporate a step in the
        collation process, in which the user can examine the preliminary collation result, edit and augment it according
        to her knowledge and possibly feed it back into the collator for another run yielding enhanced results.
    </p>

    <h3>Visualization</h3>

    <p>The last module of the Gothenburg model deals with visualizing collation results. As we are concerned with
        modelling and encoding textual variance properly, the question of how to visualize it is of technical importance
        and should not be disregarded, but is essentially out of scope with regard to this discussion.
    </p>

    <h2 id="variant-graphs" class="clear">Variant Graphs: A Data Model for Textual Variance</h2>

    <p>In a <a href="http://dx.doi.org/10.1016/j.ijhcs.2009.02.001" class="external text" rel="nofollow">recent paper</a> D. Schmidt and R. Colomb proposed a data model of textual variance (or “multi-version texts” as they call it in the title), which they call a <i>variant graph</i>:
    </p>

    <div class="figure"><img src="/images/variant-graph-schmidt.png" alt="Schmidt's Variant Graph Model"></div>

    <p>In this model, varying texts/ a collation are/ is expressed in a <a href="http://en.wikipedia.org/wiki/Directed_acyclic_graph" class="external text" rel="nofollow">directed acyclic graph</a> with each path through the graph representing one version/ witness. The textual data is annotated on the edges, each edge carrying a (common) segment of text(s) and a set of identifiers, that denotes the versions/witnesses, in which the segment appears. Transpositions can be superimposed on the graph by linking edges of transposed segments.
    </p>

    <p>The tabular model described above and the given graph-based model can be converted into each other, with Schmidt’s model having the advantage, that it is
    </p>

    <ul><li> more space-efficient as it combines matching segments into a single edge instead of duplicating them per row/column,
        </li><li> more natural in expressing transpostions as matching segments are linked and not pairs of tokens.
        </li></ul>

    <p>The tabular model on the other hand might be advantageous, if one wanted to keep collation results in a relational datastore.
    </p>

    <h2 id="alignment-algorithms">Alignment Algorithms</h2>

    <h3 id="dekker-algorithm">Dekker</h3>

    <h3 id="needleman-wunsch-algorithm">Needleman-Wunsch</h3>

    <h3 id="medite-algorithm">MEDITE</h3>

    <h2 id="output-formats">Output Formats</h2>

    <h3 id="json-output">JSON</h3>

    <p>The tabular output format, resembling matrices commonly used in <a href="http://en.wikipedia.org/wiki/Sequence_alignment" title="Wikipedia">sequence alignment</a> results, looks as follows (indentation/whitespace added for easier readability):</p>

    <pre class="prettyprint">{
    "rows": 3,
    "columns": 2,
    "sigils": ["A", "B"],
    "table":[
      [
        [ {"t":"A"} ],
        [ {"t":"A"} ]
      ],[
        [ {"t":"black"} ],
        [ {"t":"white"} ]
      ], [
        [ {"t":"cat"} ],
        [ {"t":"kitten.", "n":"cat"} ]
      ]
    ]
}</pre>

    <p>First 3&nbsp;properties: metadata … dimensions of table; <code>rows</code> aka. number of aligned segments; <code>columns</code> aka. number of witnesses;
        <code>sigils</code> contains ordered set of witness identifiers, maps to order of segments in table …</p>

    <p>Each row: matching segments, each colum/cell: the segment of a witness aka. a list of tokens comprising this segment or <code>null</code> in case of a gap …</p>

    <h3 id="tei-p5-output">TEI P5</h3>

    <pre class="prettyprint">&lt;?xml version='1.0' encoding='UTF-8'?&gt;
&lt;cx:apparatus
  xmlns:cx="http://interedition.eu/collatex/ns/1.0"
  xmlns="http://www.tei-c.org/ns/1.0"&gt;
    A
    &lt;app&gt;
      &lt;rdg wit="A"&gt;black&lt;/rdg&gt;
      &lt;rdg wit="B"&gt;white&lt;/rdg&gt;
    &lt;/app&gt;
    &lt;app&gt;
      &lt;rdg wit="A"&gt;cat&lt;/rdg&gt;
      &lt;rdg wit="B"&gt;kitten.&lt;/rdg&gt;
    &lt;/app&gt;
&lt;/cx:apparatus&gt;</pre>


    <h3 id="graphml-output">GraphML</h3>

    <p>The GraphML-formatted output of a variant graph is suitable for import of (possibly larger) graphs in tools
        for complex graph analysis and visualization, e. g. <a href="http://gephi.org/" title="Homepage">Gephi</a>.
        For an example GraphML document, take a look at sample output from the
        <a href="${cp}/collate/console" title="CollateX Console">web console</a>.</p>

    <h3 id="graphviz-dot-output">GraphViz DOT</h3>

    <pre class="prettyprint">digraph G {
  v301 [label = ""];
  v303 [label = "A"];
  v304 [label = "black"];
  v306 [label = "white"];
  v305 [label = "cat"];
  v302 [label = ""];
  v301 -> v303 [label = "A, B"];
  v303 -> v304 [label = "A"];
  v303 -> v306 [label = "B"];
  v304 -> v305 [label = "A"];
  v306 -> v305 [label = "B"];
  v305 -> v302 [label = "A, B"];
}</pre>

    <h2 id="cli">The Command Line Interface</h2>

    <pre class="prettyprint lang-xml">usage: collatex [&lt;options>] &lt;witness_1> &lt;witness_2> [[&lt;witness_3>] ...]
  -a,--algorithm &lt;arg>           progressive alignment algorithm to use
                                 'dekker' (default), 'medite',
                                 'needleman-wunsch'
  -f,--format &lt;arg>              result/output format: 'csv', 'dot',
                                 'graphml', 'tei'
  -h,--help                      print usage instructions (which your are
                                 looking at right now)
  -ie,--input-encoding &lt;arg>     charset to use for decoding non-XML
                                 witnesses; default: UTF-8
  -o,--output &lt;arg>              output file; '-' for standard output
                                 (default)
  -oe,--output-encoding &lt;arg>    charset to use for encoding the output;
                                 default: UTF-8
  -s,--script &lt;arg>              ECMA/JavaScript resource with functions to be
                                 plugged into the alignment algorithm
  -t,--tokenized                 consecutive matches of tokens will *not* be
                                 joined to segments
  -xml,--xml-mode                witnesses are treated as XML documents
  -xp,--xpath &lt;arg>              XPath 1.0 expression evaluating to tokens of
                                 XML witnesses; default: '//text()'
</pre>

    <h3 id="cli-js-callbacks">ECMA/JavaScript Callbacks</h3>

    <h2 id="rest-service">The RESTful Web Service</h2>
    <p>
        This page documents the
        <a href="http://en.wikipedia.org/wiki/Application_programming_interface" title="Wikipedia Page">Application Programming Interface (API)</a>
        of CollateX via which you can provide textual versions (&ldquo;witnesses&rdquo;) to be compared and get the collation result back in a number of formats.
    </p>

    <p>
        The CollateX service is callable via
        <a href="http://www.w3.org/Protocols/rfc2616/rfc2616-sec9.html" title="RFC">HTTP POST requests</a> to
        <a href="${cp}/collate" title="REST-API Endpoint">${cp}/collate</a>.</p>

    <h3>Input</h3>

    <p>It expects <strong>input</strong> formatted in <a href="http://json.org/" title="Website">JavaScript Object Notation (JSON)</a> as the request body;
        accordingly the content type of the HTTP request must be set to <code>application/json</code> by the client.</p>

    <p>
        An exemplary request’s body looks as follows:
    </p>

	<pre class="prettyprint">{
  "witnesses" : [ {
      "id" : "A",
      "content" : "A black cat in a black basket"
    }, {
      "id" : "B",
      "content" : "A black cat in a black basket"
    }, {
      "id" : "C",
      "content" : "A striped cat in a black basket"
    }, {
      "id" : "D",
      "content" : "A striped cat in a white basket"
    } ]
}</pre>

    <p>
        Each request body contains a single object. It has a required property <code>witnesses</code>, which contains a list of objects in turn.
        Each object represents a text version to be collated and has to contain a unique identifier in the property <code>id</code>.
        Besides the identifier each object can either contain the textual content of the witness as a string property named <code>content</code> (as shown above).
        The other option is a pre-tokenized witness, that is comprised of list of tokens notated as follows:
    </p>

    <p>
	<pre class="prettyprint">{
  "witnesses" : [ {
      "id" : "A",
      "tokens" : [
          { "t" : "A" },
          { "t" : "black" },
          { "t" : "cat" } ]
    }, {
      "id" : "B",
      "tokens" : [
          { "t" : "A" },
          { "t" : "white" },
          { "t" : "kitten.", "n" : "cat" } ]
    } ]
}</pre>
    </p>

    <p>
        Each token object has to contain a property <code>t</code>, which contains the token content itself.
        Optionally a “normalized” version of the token can be provided in the property <code>n</code>.
        It can be any kind of alternate reading the collator should use in precendence over the original token content during alignment.
        Apart from these 2&nbsp;known properties, token objects can contain an arbitrary number of additional properties,
        which will not be interpreted by the collator but just be passed through and reappear in the output.
    </p>

    <p>… Choose alignment algorithm … </p>

        <pre class="prettyprint">{
  "witnesses": [ … ],
  "algorithm": "dekker"
}</pre>

    <p>Available algorithms …</p>

    <table>
        <tr>
            <th>dekker</th>
            <td><a href="http://www.huygens.knaw.nl/en/dekker/" title="Homepage">Ronald Haentjens Dekker's</a> algorithm, includes transposition detection …</td>
        </tr>
        <tr>
            <th>dekker-experimental</th>
            <td>Experimental version of Dekker's algorithm with enhanced token matching, alpha-level quality …</td>
        </tr>
        <tr>
            <th>needleman-wunsch</th>
            <td>Adaptation of <a href="http://en.wikipedia.org/wiki/Needleman%E2%80%93Wunsch_algorithm" title="Wikipedia">Needleman-Wunsch's</a> global alignment algorithm …</td>
        </tr>
    </table>

    <p>Choose token matching function …</p>

        <pre class="prettyprint">{
  "witnesses": [ … ],
  "algorithm": "dekker",
  "tokenComparator": { type: "equality" }
}</pre>

    <p>Default: exact matching … Alternative: fuzzy matching via <a href="http://en.wikipedia.org/wiki/Levenshtein_distance" title="Wikipedia">Levenshtein distance</a> threshold:</p>

        <pre class="prettyprint">{
  "witnesses": [ … ],
  "algorithm": "dekker",
  "tokenComparator": {
    "type": "levenshtein",
    "distance": 2
  }
}</pre>

    <p>Property <code>distance</code> contains maximal edit distance to be deemed a match …</p>

    <p>More options (algorithms/ matching functions) added in forthcoming releases …</p>

    <h3>Output</h3>

    <p>
        The output format of the collator, contained in the response to an HTTP POST request, can be chosen via
        an <code>Accept</code> HTTP header in the request. The following output formats are supported:
    </p>

    <table>
        <tr>
            <th>application/json</th>
            <td><em>(per default)</em> the tabular alignment of the witnesses' tokens, represented in JSON</td>
        </tr>
        <tr>
            <th>application/tei+xml</th>
            <td>the collation result as a list of critical apparatus entries, encoded in <a href="http://www.tei-c.org/release/doc/tei-p5-doc/en/html/TC.html" title="TEI website">TEI P5 parallel segmentation mode</a></td>
        </tr>
        <tr>
            <th>application/graphml+xml</th>
            <td>the variant graph, represented in <a href="http://graphml.graphdrawing.org/">GraphML format</a></td>
        </tr>
        <tr>
            <th>text/plain</th>
            <td>the variant graph, represented in <a href="http://www.graphviz.org/doc/info/lang.html" title="Graphviz' DOT Language Specification">Graphviz' DOT Language</a></td>
        </tr>
        <tr>
            <th>image/svg+xml</th>
            <td>the variant graph, rendered as an <a href="http://www.w3.org/Graphics/SVG/" title="W3C SVG Homepage">SVG</a> vector graphics document</td>
        </tr>
    </table>

    <p>For further examples, take a look at sample output from the
        <a href="${cp}/collate/console" title="CollateX Console">web console</a>.</p>

        <div id="js-apidocs">
            <h2>The HTTP-based JavaScript API</h2>

            <p>Enables the use of CollateX' RESTful API via JavaScript … Based on <a href="http://yuilibrary.com/" title="Homepage">YUI framework</a> …</p>

            <h3>Requirements</h3>

            <p>Add dependencies to header … YUI library plus CollateX module …</p>

        <pre class="prettyprint">&lt;script type="text/javascript" src="http://yui.yahooapis.com/3.8.1/build/yui/yui-min.js">&lt;/script>
&lt;script type="text/javascript" src="http://collatex.net/demo/collatex.js">&lt;/script>
</pre>

            <p>Substitute URL prefix <code>[ROOT]</code> with the base URL of your installation, e.g.
                <a href="${cp}/" title="Base URL">this one</a> for the installation you are currently looking at …</p>

            <p>YUI module <code>interedition-collate</code> available now … supports cross-domain AJAX requests via
                <a href="http://en.wikipedia.org/wiki/Cross-Origin_Resource_Sharing" title="Wikipedia">CORS</a> …</p>

            <h3>Sample usage</h3>

        <pre class="prettyprint">YUI().use("node", "collatex", function(Y) {
    new Y.CollateX({ serviceUrl: "http://collatex.net/demo/collate" }).toTable([{
        id: "A",
        content: "Hello World"
    }, {
        id: "B",
        tokens: [
            { "t": "Hallo", "n": "hello" },
            { "t": "Welt", "n": "world" }
        ]
    }], Y.one("#result"));
});</pre>

            <p>… <code>toTable()</code> takes witness array as first parameter; second parameter is DOM node which serves as container for
                the resulting HTML alignment table …</p>

            <p>… generic <code>collate(witnesses, callback)</code> as well as methods for other formats available:
                <code>toSVG()</code>, <code>toTEI()</code>, <code>toGraphViz()</code> …</p>

            <p>… configuration of a collator instance via methods like <code>withDekker()</code>, <code>withFuzzyMatching(maxDistance)</code> …</p>
        </div>

    <h2 id="javadoc">API Documentation (Javadoc)</h2>

    <p><a href="/apidocs/" title="CollateX API (Javadoc)">here</a></p>

    <h2 id="bibliography">Resources/ Bibliography</h2>

    <ul>
        <li> Multi-Version Document Format. <a href="http://dx.doi.org/10.1016/j.ijhcs.2009.02.001"
                                               class="external text" rel="nofollow">Schmidt, D. and Colomb, R, 2009. A
                data structure for representing multi-version texts online, International Journal of Human-Computer
                Studies, 67.6, 497-514.</a> (See the related <a href="http://multiversiondocs.blogspot.com/"
                                                                class="external text" rel="nofollow">blog</a> and <a
                    href="http://multiversiondocs.blogspot.com/2008/03/whats-multi-version-document.html"
                    class="external text" rel="nofollow">the post “What's a Multi-Version Document?”</a>.
        </li>
        <li> Multi-Version Texts and Collation. <a
                    href="http://www.balisage.net/Proceedings/vol3/html/Schmidt01/BalisageVol3-Schmidt01.html"
                    class="external text" rel="nofollow">Schmidt, Desmond. “Merging Multi-Version Texts: a Generic
                Solution to the Overlap Problem.”</a> Presented at Balisage: The Markup Conference 2009, Montréal,
            Canada, August 11 - 14, 2009. In Proceedings of Balisage: The Markup Conference 2009. Balisage Series on
            Markup Technologies, vol. 3 (2009). doi:10.4242/BalisageVol3.Schmidt01.]
        </li>

        <li> Matthew Spencer, Christopher J. Howe. Collating Texts Using Progressive Multiple Alignment. Computers and
            the Humanities. 38/2004. S. 253–270.
        </li>
        <li><a href="http://edoc.bbaw.de/volltexte/2007/516/" class="external text" rel="nofollow">Michael Stolz,
                Friedrich Michael Dimpel. Computergestützte Kollationierung und ihre Integration in den editorischen
                Arbeitsfluss. 2006.</a>
        </li>

        <li><a href="http://www.sd-editions.com/about/index.html" class="external text" rel="nofollow">Collate</a>
        </li>
        <li><a href="http://collatex.sourceforge.net/" class="external text" rel="nofollow">CollateX</a>
        </li>
        <li><a href="http://www.juxtasoftware.org/" class="external text" rel="nofollow">Juxta</a>
        </li>
        <li><a href="http://code.google.com/p/multiversiondocs/wiki/NMerge" class="external text"
               rel="nofollow">NMerge</a>
        </li>
    </ul>
    <script type="text/javascript" src="/toc.js"></script>
{% endblock %}